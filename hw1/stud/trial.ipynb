{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqjmfguKUQkQ",
    "outputId": "b7ac7a78-e47a-40bd-ff48-25c1676ac694"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpcAbVlrUVij",
    "outputId": "f92e2100-1486-4464-bb03-1a6dec3abcfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/michele/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/michele/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/michele/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/michele/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/michele/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "hw_prefix = '/content/drive/MyDrive/nlp2022-hw1-main/nlp2022-hw1-main/hw1/stud'\n",
    "hw_prefix = '.'\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import sklearn\n",
    "\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_tokens = set(stopwords.words('english'))\n",
    "punc_tokens = set(punctuation)\n",
    "stop_tokens.update(punc_tokens)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#setting the embedding dimension\n",
    "EMBEDDING_DIM=50\n",
    "\n",
    "SENTENCE_MAX_LEN=50\n",
    "\n",
    "#specify the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "#setting unknown token  to handle out of vocabulary words\n",
    "UNK_TOKEN = '<unk>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "TRAIN_PATH = hw_prefix+\"/../../data/train.tsv\"\n",
    "DEV_PATH = hw_prefix+\"/../../data/dev.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Tz7lWz3_YT6K"
   },
   "outputs": [],
   "source": [
    "#creating a vocabulary with glove embeddings\n",
    "def create_glove(embedding_dim=EMBEDDING_DIM):\n",
    "    f = open(hw_prefix+'/glove/glove.6B.' + str(embedding_dim) + 'd.txt', 'r')\n",
    "    glove = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        embedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        glove[word] = embedding\n",
    "    return glove\n",
    "\n",
    "#creating a vector of word embeddings and a dictionary to pair each word with an index\n",
    "#the vector of embeddings will be needed in the embedding layer of the neural network\n",
    "def create_embeddings(vocabulary,embedding_dim=EMBEDDING_DIM):\n",
    "    vectors = []                                #creating a vector to append the vectors corresponding to words\n",
    "    word2idx = dict()                           #creating a dictionary to associate each word with an index\n",
    "    vectors.append(torch.rand(embedding_dim))   #creating a random vector for unknown (out of vocabulary) words\n",
    "    vectors.append(torch.rand(embedding_dim))   #creating a random vector for padding\n",
    "    word2idx[UNK_TOKEN] = 0                     #setting the index of the unknown token to 0\n",
    "    for word,vector in vocabulary.items():      #creating the word:index entry and insert in vectors\n",
    "        word2idx[word] = len(vectors)           #the word vector at the corresponding index for each word\n",
    "        vectors.append(torch.tensor(vector))    #in the dictionary\n",
    "    word2idx = defaultdict(lambda: 0, word2idx) #if the word we're looking for is not in the dictionary, we give the unknown token index\n",
    "    vectors = torch.stack(vectors).to(device).float()   #convert the list of tensors into a tensor of tensors\n",
    "    return vectors,word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "LPK9wa2OcRKc",
    "outputId": "cf1fdf44-1862-4fe9-88af-4278eecc1cf5",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400003\n",
      "CREATED VOCABULARY\n",
      "CREATED MODEL\n",
      "CREATED TRAIN DATASET\n",
      "CREATED DEV DATASET\n",
      "CREATED DATALOADERS\n",
      "Epoch 0   values on training set are {'mean_loss': 1.1052525237366393, 'f1': 0.1041451416374298}\n",
      "            final values on the dev set are {'mean_loss': 1.1032023327833427, 'f1': 0.3704515175147653}\n",
      "Epoch 1   values on training set are {'mean_loss': 0.906691281743102, 'f1': 0.3043937499771196}\n",
      "            final values on the dev set are {'mean_loss': 0.90175055598914, 'f1': 0.30324321797779397}\n",
      "------------------------------PATIENCE---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self,vectors,word2idx,sentences_path=None,sentences=None,lemmatization=True,\n",
    "                 test=False,class2id={\"O\": 0, \"B-PER\": 1, \"B-LOC\": 2, \"B-GRP\": 3, \"B-CORP\": 4, \"B-PROD\": 5, #indexing output classes\n",
    "                    \"B-CW\": 6, \"I-PER\": 7, \"I-LOC\": 8, \"I-GRP\": 9, \"I-CORP\": 10, \"I-PROD\": 11, \"I-CW\": 12, \"<pad>\":13}):\n",
    "        file_output = self.read_file(sentences_path) if sentences_path else self.read_sentences(sentences)\n",
    "        self.embedding_vectors = vectors\n",
    "        self.word2idx = word2idx\n",
    "        self.test = test\n",
    "        self.w_lemmatization = lemmatization\n",
    "        self.word_count = dict()\n",
    "        self.extract_sentences(file_output)\n",
    "        #self.remove_most_frequent(percentage=1)\n",
    "        # encoding of classes\n",
    "        self.class2id = class2id\n",
    "        self.id2class = {v: k for (k, v) in self.class2id.items()}\n",
    "\n",
    "    #function to remove the most frequent words in the corpus (ideally the less discriminative, only acting as noise, but\n",
    "    #it didn't work great so I'm not sure\n",
    "    def remove_most_frequent(self,percentage):\n",
    "        #print(\"STARTING MOST FREQUENT\")\n",
    "        sorted_counts = sorted(self.word_count.items(),key=lambda x:x[1],reverse=True)  #sort dict with word counts\n",
    "        total_words = len(sorted_counts)                                                #check the total number of words\n",
    "        start_index = int((total_words/100)*percentage)                                 #compute the starting point to\n",
    "        sorted_counts = sorted_counts[start_index:]                                     #exclude the wanted percentage\n",
    "        sorted_counts = set([x[0] for x in sorted_counts])                              #transforming in set to make\n",
    "        sentences = []                                                                  #checking the presence efficient (O(1))\n",
    "        #print(\"STARTING REMOVING\")\n",
    "        for sample in self.sentences:\n",
    "            sentence = sample[0]\n",
    "            new_sentence = []\n",
    "            for word,pos in sentence:\n",
    "                if word in sorted_counts:\n",
    "                    new_sentence.append((word,pos))\n",
    "            sentences.append((new_sentence,sample[1],sample[2]))\n",
    "        self.sentences = sentences\n",
    "\n",
    "\n",
    "    #little function to read a file given the path\n",
    "    def read_file(self,path):\n",
    "        sentences = list()\n",
    "        with open(path) as file:\n",
    "            tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "            for idx,line in enumerate(tsv_file):\n",
    "                if len(line) > 0:\n",
    "                    if line[0] == '#':\n",
    "                        \n",
    "                        sentences.append(dict())\n",
    "                        sentences[-1][\"id\"] = line[2]\n",
    "                        sentences[-1][\"text\"] = []\n",
    "                        sentences[-1][\"labels\"] = []\n",
    "                    else:\n",
    "                        sentences[-1][\"text\"].append(line[0])\n",
    "                        sentences[-1][\"labels\"].append(line[1])\n",
    "        #print([len(sentences[i]['text']) for i in range(len(sentences))])\n",
    "        return sentences\n",
    "\n",
    "    def read_sentences(self,sentences):\n",
    "        sents = list()\n",
    "        for idx,line in enumerate(sentences):\n",
    "            d = dict()\n",
    "            d[\"id\"] = idx\n",
    "            d[\"text\"] = line\n",
    "            d[\"labels\"] = [\"O\" for token in line]\n",
    "            sents.append(d)\n",
    "        return sents\n",
    "\n",
    "    #function to extract the sentences from the dictionary of samples\n",
    "    def extract_sentences(self,file_output):\n",
    "        self.sentences = list()                 #creating a list to store the instances in the dataset\n",
    "        for instance in file_output:\n",
    "            processed = self.text_preprocess(instance)  #preprocessing of the sentence\n",
    "            labels = 'UNKNOWN'   #this is needed to make the system able to give a prediction without having a ground truth\n",
    "            if 'labels' in instance: #then if there is a ground truth we take it\n",
    "                labels = processed['labels']\n",
    "            self.sentences.append((processed[\"text\"], labels, id))           #append a triple (sentence,label,id) which are all the informations we need\n",
    "        if not self.test: random.Random(42).shuffle(self.sentences)         #for the training phase, shuffle data to avoid bias relative to data order\n",
    "        #print(self.sentences)\n",
    "\n",
    "    #function to convert the pos extracted by nltk to the pos required by the very same library for lemmatization\n",
    "    #I also use it to give pos='' to punctuation\n",
    "    def get_standard(self,pos):\n",
    "        if pos[0] == 'V': return wordnet.VERB\n",
    "        if pos[0] == 'R': return wordnet.ADV\n",
    "        if pos[0] == 'N': return wordnet.NOUN\n",
    "        if pos[0] == 'J': return wordnet.ADJ\n",
    "        return ''\n",
    "\n",
    "    #function which includes all the preprocessing steps for the sentences, which are tokenization,\n",
    "    #stopwords and punctuation removal,pos tagging and lemmatization\n",
    "    def text_preprocess(self,sentence):\n",
    "        text = sentence[\"text\"]\n",
    "        labels = sentence[\"labels\"]\n",
    "        sent = [(text[i],labels[i]) for i in range(len(text))]# if text[i] not in string.punctuation and text[i] not in stop_tokens]\n",
    "        #sent = [(text[i],labels[i]) for i in range(len(text)) if labels[i] != 'O']\n",
    "        sentence[\"text\"] = [pair[0] for pair in sent]\n",
    "        sentence[\"labels\"] = [pair[1] for pair in sent]\n",
    "        return sentence\n",
    "        '''\n",
    "        standard_tokens = [(token,self.get_standard(pos)) for token,pos in tokens_n_pos]\n",
    "        clean_standard = [(token,pos) for token,pos in standard_tokens if pos != '']   #light stopwords removal\n",
    "        clean_standard2 = [(token, pos) for token, pos in standard_tokens if token not in stop_tokens]  # full stopwords removal\n",
    "        if self.w_lemmatization:            #choosing if applying lemmatization\n",
    "            lemmatized = [(lemmatizer.lemmatize(token.lower(),pos),pos) if pos != '' else (lemmatizer.lemmatize(token.lower()),'') for token,pos in clean_standard2]\n",
    "            #print(\"STARTED BUILDING WORD COUNT\")\n",
    "            for lemma,pos in lemmatized:\n",
    "                if lemma in self.word_count:\n",
    "                    self.word_count[lemma] += 1\n",
    "                else:\n",
    "                    self.word_count[lemma] = 1\n",
    "            return lemmatized\n",
    "        return [(word,pos) for word,pos in clean_standard2]\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #function to return the number of instances contained in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    #function to get the i-th instance contained in the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "    #custom dataloader which incorporates the collate function\n",
    "    def dataloader(self,batch_size):\n",
    "        return DataLoader(self,batch_size=batch_size,collate_fn=partial(self.collate,self.word2idx))\n",
    "\n",
    "        #function to map each lemma,pos in a sentence to their indexes\n",
    "    def sent2idx(self ,sent, word2idx):\n",
    "        #return torch.tensor([word2idx[word] for word,pos in sent]) #in the case i'm not using pos I just return a placeholder for pos\n",
    "        return torch.tensor([word2idx[word] for word in sent])\n",
    "\n",
    "\n",
    "\n",
    "    #custom collate function, used to create the batches to give as input to the nn\n",
    "    #it's needed because we are dealing with sentences of variable length and we need padding\n",
    "    #to be sure that each sentence in a batch has the same length, which is necessary since\n",
    "    #neural networks need fixed dimension inputs\n",
    "    def collate(self,word2idx, data):\n",
    "        X = [self.sent2idx(instance[0], word2idx) for instance in data]                             #extracting the input sentence\n",
    "        X_len = torch.tensor([x.size(0) for x in X], dtype=torch.long).to(device)\n",
    "        y = [self.sent2idx(instance[1], self.class2id) for instance in data]\n",
    "        ids = [instance[2] for instance in data]                                                    #extracting the sentence ids\n",
    "        X = torch.nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=1).to(device)        #padding all the sentences to the maximum length in the batch (forcefully max_len)\n",
    "        y = torch.nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=self.class2id[PAD_TOKEN]).to(device)              #extracting the ground truth\n",
    "        return X, X_len,y, ids\n",
    "\n",
    "\n",
    "#Model class\n",
    "#class StudentModel(nn.Module,Model):        #needed for testing\n",
    "class StudentModel(nn.Module):             #needed for training\n",
    "\n",
    "    #constructor method, for which are needed the word embedding vectors, the dimensions of the two linear\n",
    "    #layers, the dropout probabilty p, a flag to choose if the lstm layer must be bidirectonal, the number\n",
    "    #of layers of the lstm layer and the loss function (but these last 4 already have a default value)\n",
    "    def __init__(self,embeddings,   #word embedding vectors\n",
    "                 pos_embeddings=None,    #pos embedding vectors\n",
    "                 hidden1=128,           #dimension of the first hidden layer\n",
    "                 hidden2=128,           #dimension of the second hidden layer\n",
    "                 hidden3=128,\n",
    "                 p=0.0,             #probability of dropour layer\n",
    "                 bidirectional=False,   #flag to decide if the LSTM must be bidirectional\n",
    "                 lstm_layers=1,         #layers of the LSTM\n",
    "                 loss_fn=torch.nn.CrossEntropyLoss(ignore_index=0),\n",
    "                 num_classes=13):   #loss function\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings,freeze=False)\n",
    "        self.pos_embeddings = None if pos_embeddings == None else nn.Embedding.from_pretrained(pos_embeddings,freeze=False)     #choose if creating or not an embedding layer\n",
    "                                                                               #based on wether pos embeddings were created\n",
    "        input_dim = embeddings.size(1) if not self.pos_embeddings else embeddings.size(1)+pos_embeddings.size(1)                #or not\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden1, num_layers=lstm_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        hidden1 = hidden1*2 if bidirectional else hidden1\n",
    "        self.lin1 = nn.Linear(hidden1, hidden2)\n",
    "        self.lin2 = nn.Linear(hidden2,hidden3)\n",
    "        self.lin3 = nn.Linear(hidden3,num_classes)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.num_classes = num_classes\n",
    "    #forward method, automatically called when calling the instance\n",
    "    #it takes the Xs and their length in batches\n",
    "    def forward(self,X,X_len):\n",
    "        #lemmas,pos = X[...,0],X[...,1]                                  #separating pos from lemmas\n",
    "        embeddings = self.embedding(X)                                #expanding the words from indices to embedding vectors\n",
    "        '''\n",
    "        if self.pos_embeddings != None:                                     #in the case I'm using pos embeddings, I pass their indexes through their own embedding layer\n",
    "            pos_embeddings_out1 = self.pos_embeddings(pos)                 #and then concatenate them to the corresponding words\n",
    "            \n",
    "            embedding1 = torch.cat([embedding,pos_embeddings_out1],dim=-1)\n",
    "        '''\n",
    "        #embeddings = embeddings.unsqueeze(1)\n",
    "        lstm_out = self.lstm(embeddings)[0]\n",
    "        lstm_out = lstm_out.squeeze()\n",
    "        batch_size, sentence_len, hidden_size = lstm_out.shape  #sentence length here is taken to remove padding\n",
    "        flattened1 = lstm_out.reshape(-1, hidden_size)        #the output of the lstm is flattened (batch size,sentence length,lstm output dimension)->\n",
    "                        #                                       (batch size,sentence length*lstm output dimension)\n",
    "        last_word_relative_indices = X_len - 1                #in order to have the last index of each sentence in the batch (excluding padding), we subtract one to the length\n",
    "        sentences_offsets = torch.arange(batch_size, device=device) * sentence_len   #we compute the offsets, or absolute indexes, where each sentence starts\n",
    "        vec_sum_index = sentences_offsets + last_word_relative_indices     #we take the last token of each sentence, which is the hidden\n",
    "                                                                            #state of the LSTM which summarize the content of the sentence\n",
    "        #vec_sum = flattened1[vec_sum_index]                         #we use the index to retrieve the vector which summarizes the sentence\n",
    "        ##out = self.dropout(torch.cat([vec_sum1,vec_sum2],dim=-1))\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = torch.relu(out)\n",
    "        #print(out.size())\n",
    "        out = self.lin1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.lin3(out)\n",
    "        out = out.squeeze(1)\n",
    "        #print(out.size())\n",
    "        out = torch.softmax(out,dim=-1) if self.num_classes > 1 else torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "    def predict(self, tokens: List[List[str]]) -> List[List[str]]:\n",
    "        # STUDENT: implement here your predict function\n",
    "        # remember to respect the same order of tokens!\n",
    "        batch_size = 32\n",
    "        predictions = list()\n",
    "        dataset = SentenceDataset(sentences=tokens, vectors=embeddings, word2idx=word2idx, test=True)\n",
    "        dataloader = dataset.dataloader(batch_size)\n",
    "        for batch in dataloader:\n",
    "            batch_x = batch[0]\n",
    "            batch_xlen = batch[1]\n",
    "            ids = batch[3]\n",
    "            logits = self.forward(batch_x, batch_xlen)\n",
    "            logits = logits.view(-1, logits.shape[-1])\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            preds = torch.reshape(preds, (batch_x.size(0), -1))\n",
    "            for i in range(len(batch_x)):\n",
    "                prediction = []\n",
    "                for j in range(len(batch_x[i])):\n",
    "                    if j < batch_xlen[i]:\n",
    "                        # print(preds)\n",
    "                        # print(preds[i])\n",
    "                        prediction.append(dataset.id2class[preds[i][j].item()])\n",
    "                predictions.append(prediction)\n",
    "\n",
    "        print(len(tokens), len(predictions))\n",
    "        print(tokens)\n",
    "        print(predictions)\n",
    "        return predictions\n",
    "\n",
    "#trainer class\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self,model,optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    #train function, takes two dataloaders for trainset and devset in order to train on the trainset and\n",
    "    #check at every epoch how the training is affecting performance on the dev set, to avoid overfitting\n",
    "    #I use the patience mechanism to stop after 5 times the accuracy on devset goes down, since I noticed\n",
    "    #that after that point it just gets worse\n",
    "    def train(self,train_loader, dev_loader,patience, epochs=10):\n",
    "        loss_history = [[], []]             #lists to save trainset and devset loss in order to plot the graph later\n",
    "        f1_history = [[], []]         #lists to save trainset and devset accuracy in order to plot the graph later\n",
    "        patience_counter = 0                #counter to implement patience\n",
    "        for i in range(epochs):\n",
    "            losses = []                     #list to save the loss for each batch\n",
    "            hit = 0                         #counter for correct predictions\n",
    "            total = 0                       #counter for all predictions\n",
    "            f1s = []\n",
    "            for batch in train_loader:\n",
    "                batch_x = batch[0]         #separating first from second sentences\n",
    "                batch_xlen = batch[1]      #separating lengths of first and second sentences\n",
    "                labels = batch[2]          #taking the ground truth\n",
    "                ids = batch[3]\n",
    "                self.optimizer.zero_grad()  #setting the gradients to zero for each batch\n",
    "                logits = self.model(batch_x, batch_xlen) #predict\n",
    "                logits = torch.flatten(logits) if self.model.num_classes == 1 else logits.view(-1, logits.shape[-1])\n",
    "                \n",
    "                labels = labels.view(-1)\n",
    "                #print(logits.size(),labels.size())\n",
    "                #print(logits)\n",
    "                #print(labels)\n",
    "                loss = self.model.loss_fn(logits.float(), labels.float()) if self.model.num_classes == 1 else  self.model.loss_fn(logits, labels)             #calculating the loss\n",
    "                #max_logits = \n",
    "                edit_logits = torch.round(logits) if self.model.num_classes == 1 else torch.argmax(logits, dim=1)\n",
    "                average = \"binary\" if self.model.num_classes == 1 else \"macro\"\n",
    "                f1s.append(sklearn.metrics.f1_score(labels.cpu().detach().numpy(),edit_logits.cpu().detach().numpy(),average=average,\n",
    "                                                    labels=range(self.model.num_classes-1)))\n",
    "                '''\n",
    "                for j in range(len(logits)):                                      #checking the number of hits in order to compute accuracy\n",
    "                    total += 1\n",
    "                    if torch.argmax(logits[j]) == labels[j]: hit += 1\n",
    "                '''\n",
    "                loss.backward()             #backpropagating the loss\n",
    "                self.optimizer.step()       #adjusting the model parameters to the loss\n",
    "                losses.append(loss.item())  #appending the losses to losses\n",
    "            '''\n",
    "            accuracy = hit/total            #computing accuracy\n",
    "            accuracy_history[0].append(accuracy)    #appending accuracy to accuracy history\n",
    "            '''\n",
    "            f1 = sum(f1s)/len(f1s)\n",
    "            f1_history[0].append(f1)\n",
    "            mean_loss = sum(losses) / len(losses)   #computing the mean loss for each epoch\n",
    "            loss_history[0].append(mean_loss)       #appending the mean loss of each epoch to loss history\n",
    "            metrics = {'mean_loss': mean_loss, 'f1': f1}    #displaying results of the epoch\n",
    "            print(f'Epoch {i}   values on training set are {metrics}')\n",
    "            #the same exact process is repeated on the instances of the devset, minus gradient backpropagation and optimization of course\n",
    "            hit = 0\n",
    "            total = 0\n",
    "            f1s = []\n",
    "            with torch.no_grad():\n",
    "                #RESET LOSSES????\n",
    "                for batch in dev_loader:\n",
    "                    batch_x = batch[0]\n",
    "                    batch_xlen = batch[1]\n",
    "                    labels = batch[2]\n",
    "                    ids = batch[3]\n",
    "                    logits = self.model(batch_x, batch_xlen)\n",
    "                  \n",
    "                    logits = torch.flatten(logits) if self.model.num_classes == 1 else logits.view(-1, logits.shape[-1])\n",
    "                    labels = labels.view(-1)\n",
    "                    edit_logits = torch.round(logits) if self.model.num_classes == 1 else torch.argmax(logits, dim=1)\n",
    "                    average = \"binary\" if self.model.num_classes == 1 else \"macro\" \n",
    "                    loss = self.model.loss_fn(logits.float(), labels.float()) if self.model.num_classes == 1 else  self.model.loss_fn(logits, labels)             #calculating the loss\n",
    "                    losses.append(loss.item())\n",
    "                    f1s.append(sklearn.metrics.f1_score(labels.cpu().detach().numpy(), edit_logits.cpu().detach().numpy()\n",
    "                                                        ,average=average,labels=range(self.model.num_classes-1)))\n",
    "                    '''\n",
    "                    for j in range(len(logits)):\n",
    "                        total += 1\n",
    "                        if torch.argmax(logits[j]) == labels[j]: hit += 1\n",
    "                    '''\n",
    "            mean_loss = sum(losses) / len(losses)\n",
    "            loss_history[1].append(mean_loss)\n",
    "            f1 = sum(f1s) / len(f1s)\n",
    "            f1_history[1].append(f1)\n",
    "            metrics = {'mean_loss': mean_loss, 'f1': f1}\n",
    "            print(f'            final values on the dev set are {metrics}')\n",
    "\n",
    "            if len(f1_history[1]) > 1 and f1_history[1][-1] < f1_history[1][-2]:\n",
    "                patience_counter += 1\n",
    "                if patience == patience_counter:\n",
    "                    print('-----------------------------EARLY STOP--------------------------------------------')\n",
    "                    break\n",
    "                else:\n",
    "                    print('------------------------------PATIENCE---------------------------------------------')\n",
    "\n",
    "        return {\n",
    "            'loss_history': loss_history,\n",
    "            'f1_history': f1_history\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def read_dataset(path: str) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "\n",
    "    tokens_s = []\n",
    "    labels_s = []\n",
    "\n",
    "    tokens = []\n",
    "    labels = []\n",
    "\n",
    "    with open(path) as f:\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith(\"#\\t\"):\n",
    "                tokens = []\n",
    "                labels = []\n",
    "            elif line == \"\":\n",
    "                tokens_s.append(tokens)\n",
    "                labels_s.append(labels)\n",
    "            else:\n",
    "                token, label = line.split(\"\\t\")\n",
    "                tokens.append(token)\n",
    "                labels.append(label)\n",
    "\n",
    "    assert len(tokens_s) == len(labels_s)\n",
    "\n",
    "    return tokens_s, labels_s\n",
    "\n",
    "#utility function to plot accuracy and loss\n",
    "def plot_logs(history,param):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    train_param = history[0]\n",
    "    test_param = history[1]\n",
    "    plt.plot(list(range(len(train_param))), train_param, label='Train '+param)\n",
    "    plt.plot(list(range(len(test_param))), test_param, label='Test '+param)\n",
    "    plt.title('Train vs Test '+param)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "model_path = hw_prefix+\"/../../model/myModel.ckpt\"\n",
    "glove = create_glove()                                          #create glove dictionary\n",
    "embeddings,word2idx = create_embeddings(glove)                  #create and indexing embeddings\n",
    "print(len(embeddings))\n",
    "print(\"CREATED VOCABULARY\")\n",
    "binaryModel = StudentModel(embeddings,bidirectional=False,hidden1=128,hidden2=128,lstm_layers=1,p=0,loss_fn=nn.CrossEntropyLoss(ignore_index=2),\n",
    "                     num_classes=3).to(device)         #instantiating the model\n",
    "\n",
    "#simpleModel = StudentModel(embeddings,bidirectional=False,hidden1=128,hidden2=128,lstm_layers=1,p=0.5,loss_fn=nn.CrossEntropyLoss(ignore_index=0),\n",
    "#                     num_classes=7).to(device)         #instantiating the model\n",
    "\n",
    "#bioModel = StudentModel(embeddings,bidirectional=True,hidden1=128,hidden2=128,hidden3=128,lstm_layers=1,p=0,loss_fn=nn.CrossEntropyLoss(ignore_index=2),\n",
    "#                     num_classes=3).to(device)         #instantiating the model\n",
    "print(\"CREATED MODEL\")\n",
    "#'''\n",
    "binaryClass2id = {\"O\":0,\"B-PER\": 1, \"B-LOC\": 1, \"B-GRP\": 1, \"B-CORP\": 1, \"B-PROD\": 1, #indexing output classes\n",
    "                    \"B-CW\": 1, \"I-PER\": 1, \"I-LOC\": 1, \"I-GRP\": 1, \"I-CORP\": 1, \"I-PROD\": 1, \"I-CW\": 1, \"<pad>\":2}\n",
    "\n",
    "simpleClass2id = {\"O\":0,\"B-PER\": 1, \"B-LOC\": 2, \"B-GRP\": 3, \"B-CORP\": 4, \"B-PROD\": 5, #indexing output classes\n",
    "                    \"B-CW\": 6, \"I-PER\": 1, \"I-LOC\": 2, \"I-GRP\": 3, \"I-CORP\": 4, \"I-PROD\": 5, \"I-CW\": 6, \"<pad>\":7}\n",
    "\n",
    "bioClass2id = {\"B-PER\": 0, \"B-LOC\": 0, \"B-GRP\": 0, \"B-CORP\": 0, \"B-PROD\": 0, #indexing output classes\n",
    "                    \"B-CW\": 0, \"I-PER\": 1, \"I-LOC\": 1, \"I-GRP\": 1, \"I-CORP\": 1, \"I-PROD\": 1, \"I-CW\": 1, \"<pad>\":2}\n",
    "\n",
    "afterClass2id = {\"B-PER\": 0, \"B-LOC\": 1, \"B-GRP\": 2, \"B-CORP\": 3, \"B-PROD\": 4, #indexing output classes\n",
    "                    \"B-CW\": 5, \"I-PER\": 6, \"I-LOC\": 7, \"I-GRP\": 8, \"I-CORP\": 9, \"I-PROD\": 10, \"I-CW\": 11, \"<pad>\":12}\n",
    "\n",
    "bin_train_dataset = SentenceDataset(sentences_path=TRAIN_PATH,vectors=embeddings,word2idx=word2idx,class2id=binaryClass2id) #instantiating the training dataset\n",
    "#simple_train_dataset = SentenceDataset(sentences_path=TRAIN_PATH,vectors=embeddings,word2idx=word2idx,class2id=simpleClass2id) #instantiating the training dataset\n",
    "#bio_train_dataset = SentenceDataset(sentences_path=TRAIN_PATH,vectors=embeddings,word2idx=word2idx,class2id=bioClass2id) #instantiating the training dataset\n",
    "print(\"CREATED TRAIN DATASET\")\n",
    "\n",
    "bin_dev_dataset = SentenceDataset(sentences_path=DEV_PATH,vectors=embeddings,word2idx=word2idx,test=True,class2id=binaryClass2id)       #instantiating the dev dataset\n",
    "#simple_dev_dataset = SentenceDataset(sentences_path=DEV_PATH,vectors=embeddings,word2idx=word2idx,test=True,class2id=simpleClass2id)       #instantiating the dev dataset\n",
    "#bio_dev_dataset = SentenceDataset(sentences_path=DEV_PATH,vectors=embeddings,word2idx=word2idx,test=True,class2id=bioClass2id)\n",
    "print(\"CREATED DEV DATASET\")\n",
    "bin_train_dl = bin_train_dataset.dataloader(batch_size=32)                         #instantiating the dataloaders\n",
    "bin_dev_dl = bin_dev_dataset.dataloader(batch_size=32)\n",
    "\n",
    "#simple_train_dl = simple_train_dataset.dataloader(batch_size=64)                         #instantiating the dataloaders\n",
    "#simple_dev_dl = simple_dev_dataset.dataloader(batch_size=64)\n",
    "\n",
    "#bio_train_dl = bio_train_dataset.dataloader(batch_size=16)                         #instantiating the dataloaders\n",
    "#bio_dev_dl = bio_dev_dataset.dataloader(batch_size=16)\n",
    "print(\"CREATED DATALOADERS\")\n",
    "optimizer = torch.optim.Adam(binaryModel.parameters(), lr=0.00001, weight_decay=0.00) #instantiating the optimizer\n",
    "trainer = Trainer(model=binaryModel,optimizer=optimizer)                                              #instantiating the trainer\n",
    "\n",
    "#optimizer = torch.optim.Adam(simpleModel.parameters(), lr=0.001, weight_decay=0.00) #instantiating the optimizer\n",
    "#trainer = Trainer(model=simpleModel,optimizer=optimizer)                                              #instantiating the trainer\n",
    "\n",
    "#optimizer = torch.optim.Adam(bioModel.parameters(), lr=0.0001, weight_decay=0.00) #instantiating the optimizer\n",
    "#trainer = Trainer(model=bioModel,optimizer=optimizer)                                              #instantiating the trainer\n",
    "\n",
    "histories = trainer.train(train_loader=bin_train_dl,dev_loader=bin_dev_dl,patience=10,epochs=50)    #training\n",
    "params = ['loss', 'f1']                                                   #plotting the metrics\n",
    "for param in params:\n",
    "    plot_logs(histories[param + '_history'], param)\n",
    "#torch.save(binaryModel.state_dict(), model_path)                                      #saving the model\n",
    "#torch.save(simpleModel.state_dict(), model_path)                                      #saving the model\n",
    "torch.save(binaryModel.state_dict(), model_path)                                      #saving the model\n",
    "\n",
    "'''\n",
    "model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "model.predict(read_dataset(DEV_PATH)[0])\n",
    "\n",
    "\n",
    "#'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZzYzz9trWUSW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "trial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}